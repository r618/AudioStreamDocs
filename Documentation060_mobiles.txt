::v3.0::1o.2o22::2019.4.0::

================================================================================
iOS Build notes:
================================================================================

- Since FMOD Version 1.09 added support for Google VR, later replaced by Google Resonance, and the respective plugin is not Bitcode enabled it is necessary to not build Xcode project with Bitcode support - or, if you don't need Resonance plugin, simply delete libresonanceaudio plugin/library.
- arbitrary loads for app transport security settings should be enabled ( newer versions of Unity handle this automatically via Allow HTTP downloads in Player settings ) in order to stream internet content via HTTP.
- Resonance plugin have to be manually enabled in fmodpluing.cpp source - see there for further reference


================================================================================
iOS recording notes:
================================================================================

- check 'Prepare iOS For Recording' in iOS Player Settings / Other settings

- ensure that DSP Bufer Size in Project Settings - Audio is set to 'Best latency' and/or possibly adjust the DSP buffer size on AudioStreamInput* component/s (the defaults work across all tested platforms, though) - otherwise recording might not work

- add 'Privacy - Microphone Usage Description' ( raw value NSMicrophoneUsageDescription ) key and its value to Info.plist ( Target / Info ) in generated Xcode project.
Newer versions of Unity allow you to specify this key in iOS Player Settings via 'Microphone usage description' ( this is needed due to privacy concerns, iOS will ask for confirmation before first usage of the microphone ).

- when 'Prepare iOS For Recording' is selected - from the manual: 'When selected, the microphone recording APIs are initialised. This makes recording latency lower, though on iPhones it re-routes audio output via earphones only.' ( note: here 'earphones' should be more likely 'earspeaker' )
Since 1.4.1 AudioStream provides 'fix' for this situation with included iOS plugin which requests an audio route override for normal playback to be on speaker/headset; recording uses normal route, i.e. recorded output is on earspeaker/default.
Newer ( 2017.1 and up ) versions of Unity provide setting for this in iOS Player Settings - 'Force iOS Speakers when Recording'; for 2017.1 and up the fix above is skipped

- FMOD currently does not provide any way of recording from other than default recording device (iPhone Microphone) - that means that it can't record from e.g. AirPods / connected Bluetooth headsets etc.

Since AudioStream 2.0 the asset provides iOS native plugin which should allow to record from any connected audio device - its demo is in 'AudioStreamInput_iOS_ExternalDevicesDemo' which shows all currently connected devices and allows any of them to be selected and to be record from:

	------------------------ !
	If you want to record from, or play on external/Bluetooth devices on iOS you have to - apart from the above - add project wide 'Scripting Define Symbol' called "AUDIOSTREAM_IOS_DEVICES" to iOS Player settings
	[ the audio session category used for BT playback is tied to recording - that's why it needs to have also recording enabled ]
	Default to speaker - which was needed in older versions of Unity - is still enabled by default ( should be possible to set it in iOS Player settings too )

	You don't need to do any of this if you just want to play audio on default/normal device output as per Unity set audio session.
	------------------------ !

	The input audio is routed to an AudioSource so can be processed as any usual Unity audio (the cubes in the scene react to an actual Unity sound as usual).
	Note it supports hotplugging (you should be able to dis/connect device while running and the change should be reflected in selectable devices), but hotplugging is not 100% reliable - initiate session update via 'Update audio session' to update device in that case
	Note that the scene will work only when run on an actual iOS device

	You can unmute the AudioSource but note that you'll get small echo in that case since the audio is by default played on earspeaker/speaker by iOS and then by Unity with small delay.


================================================================================
Background audio mode on mobiles:
================================================================================

iOS:
====
	Enable background audio on iOS application level:

		- In Unity, iOS Player Settings:

			- set 'Behavior in Background': Custom
				- enable 'Audio, AirPlay, PiP'

			this generates appropriate entries in Xcode project:
				- Application does not run in background    NO
				- Required background modes
						- Item 0                                App plays audio or streams audio/video using AirPlay

			(you can inspect them manually if needed)

	That's it, everything else should be done automatically.

	Details:
		- setup is done via a custom App Controller in 'AudioStream\Plugins\iOS\AudioStreamAppController'
		- this sets audio session for the app for iOS to play the audio while the app is in the background
			(Unity default audio session implementation isn't often consistent across Unity/iOS versions for this)
		- implements Unity player loop background refreshing (needed for all Updates in the app, so also for e.g. networking)
		- If you use more than 1 asset which implements custom UnityAppCotroller, you'd have to chain them (explicitely set descedants hierarchy manually)

		:: == the background timer is set to 1 second by default and might not be suitable for all devices. If you encounter network (and thus playback) related issues please change it manually for now at:
			Plugins\iOS\AudioStreamAppController.mm
			ín 'applicationWillResignActive'
			at 'iceTimer' creation
			change it to smaller value such as 0.3
			for scheduledTimerWithTimeInterval

Also, slight suggestion. In your AudioStreamAppController you set refresh rate for Unity in the background to 1sec. This results in constant readUnderflows even on 4g. I would suggest you add possibility to express this value in Inspector, or at least provide a bit clarification. For me values 0.3 do the trick.

	
	For full implementation including ability to start/stop playback by the user when in the background from Control centre/lock screen working remote events are needed.
	These probably will come in later update.
		This full implmentation is already present in e.g. AudioStreamIce [https://assetstore.unity.com/packages/slug/223601?aid=1100l7sC8]

Android:
========
	Background running and Media Controls on Android are normally implemented via a service, which interacts with OS audio spearately from main app/activity (Unity player), - so the asset doesn't implement this.

	For at least some functionality of the main app's background audio you can comment out onPause method of Unity player activity in Gradle build:
	(
		- Gradle build system and Android Studio are needed
		- In Unity, Build Settings:
			- select Build System: Gradle
			- Export Project checked
		- Android Studio:
			- import Project/Gradle build script
			- find and open UnityPlayerActivity.java in src/main/java folder
			- comment out onPause method
	)
	but note that
		- OS might (and probably will) kill the app while it's in the background
		- is not entirely correct way of doing things overall

		As in the case of iOS above these are the basics - you won't get music controls available for the user on the lock screen for example.
